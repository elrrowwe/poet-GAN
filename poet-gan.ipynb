{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":46011,"sourceType":"datasetVersion","datasetId":34370}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport numpy as np \nimport os\nfrom PIL import Image\nfrom torchvision.transforms import v2\nimport torchvision.transforms as t\nfrom sklearn.model_selection import train_test_split\n\n#a list of resized images from the dataset in the form of tensors\nimages = [] \n\ntransform = t.Compose([\n    t.ToTensor(),\n    v2.Resize(size=(500, 500)),\n    v2.Grayscale()])\n\n#transforming each image in the directory, appending the images to the images list\nfor root, dirs, files in os.walk(\"/kaggle/input/iam-handwriting-top50/data_subset/data_subset\", topdown=False):\n    for name in files:\n        im = Image.open(os.path.join(root, name))\n        im = transform(im)\n        images.append(im)","metadata":{"_uuid":"95e05ba8-58b1-4c3b-82c5-6284b2cbd2dc","_cell_guid":"b396deb3-a3a7-448e-a9a0-b348009ddf16","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-10T18:59:57.488319Z","iopub.execute_input":"2024-01-10T18:59:57.488706Z","iopub.status.idle":"2024-01-10T19:00:41.735093Z","shell.execute_reply.started":"2024-01-10T18:59:57.488673Z","shell.execute_reply":"2024-01-10T19:00:41.734218Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n  warnings.warn(_BETA_TRANSFORMS_WARNING)\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n  warnings.warn(_BETA_TRANSFORMS_WARNING)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"#making sure the values are between 0 and 1\nprint(images[1])","metadata":{"_uuid":"f2fe7106-33e7-41fb-813f-39a421ed54fc","_cell_guid":"8be6997f-c687-4513-89c9-ec6f126ef0df","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-10T19:00:47.722446Z","iopub.execute_input":"2024-01-10T19:00:47.722980Z","iopub.status.idle":"2024-01-10T19:00:47.765509Z","shell.execute_reply.started":"2024-01-10T19:00:47.722944Z","shell.execute_reply":"2024-01-10T19:00:47.764527Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         ...,\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.]]])\n","output_type":"stream"}]},{"cell_type":"code","source":"#splitting the images list into train, test sets\nX_train, X_test = train_test_split(images[:3000], test_size=0.3)\nprint(X_train[1].size())","metadata":{"_uuid":"42310e80-c9b6-452d-9fe6-53e164d44f29","_cell_guid":"6dad4168-ef08-4fbc-a621-4b51e6f623e6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-10T19:00:48.224091Z","iopub.execute_input":"2024-01-10T19:00:48.224817Z","iopub.status.idle":"2024-01-10T19:00:48.231429Z","shell.execute_reply.started":"2024-01-10T19:00:48.224782Z","shell.execute_reply":"2024-01-10T19:00:48.230377Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"torch.Size([1, 784, 784])\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"_uuid":"8258a8f7-3de9-4bdc-a6c5-0ea8be4b6014","_cell_guid":"9334b725-b076-4f3a-bf9e-4b2c1cbd2fe5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-10T19:00:48.813278Z","iopub.execute_input":"2024-01-10T19:00:48.813671Z","iopub.status.idle":"2024-01-10T19:00:50.333656Z","shell.execute_reply.started":"2024-01-10T19:00:48.813641Z","shell.execute_reply":"2024-01-10T19:00:50.332592Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Wed Jan 10 19:00:50 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   34C    P0              25W / 250W |      0MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, z_dim: int, out_size=784, lin1_channels: int=264, output_channels: int=1, device: str='cuda'):\n        \"\"\"\n        The constructor of the Generator part of the network.\n\n        z_dim: the dimension of the input noise vector\n        lin1_hw: the height, width of the feature map produced by the first, dense layer. defaults to 4\n        lin1_channels: the number of channels in the output of the first, dense layer. defaults to 128\n        output_channels: the number of channels in the output of the network. defaults to 1 \n        device: the device to send the calculations to. defaults to 'cuda' \n        \"\"\"\n        super(Generator, self).__init__()  \n\n        self.z_dim = z_dim\n        self.lin1_channels = lin1_channels\n        self.output_channels = output_channels\n        self.device = device\n\n        self.G = nn.Sequential(\n            nn.Linear(z_dim, lin1_channels, True, self.device),\n            nn.LeakyReLU(0.1),\n            \n            nn.Linear(lin1_channels, lin1_channels * 2, True, self.device),\n            nn.LeakyReLU(0.1),\n            \n            nn.Dropout(0.5),\n            \n            nn.Linear(lin1_channels * 2, lin1_channels * 3, True, self.device),\n            nn.LeakyReLU(0.1),\n            \n            nn.Linear(lin1_channels * 3, out_size, True, self.device),\n            nn.LeakyReLU(0.1),\n            \n            nn.Linear(out_size, out_size, True, self.device),\n            nn.Tanh()\n        )\n\n    def forward(self, z: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        The forward pass of the network.\n\n        returns a tensor (an image).\n        \"\"\"\n    \n        return self.G(z)","metadata":{"_uuid":"9d7ff82a-78fd-4c8d-bb3c-a16556a97c64","_cell_guid":"a154188a-7301-4213-86a6-d366e1782330","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-10T20:12:37.676730Z","iopub.execute_input":"2024-01-10T20:12:37.677484Z","iopub.status.idle":"2024-01-10T20:12:37.687163Z","shell.execute_reply.started":"2024-01-10T20:12:37.677447Z","shell.execute_reply":"2024-01-10T20:12:37.686113Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, input_size:int=784, conv1_channels: int=64, device: str='cuda'):\n        \"\"\"\n        The constructor of the Discriminator part of the network.\n        \n        input_channels: the number of channels in the input. defaults to 1\n        conv1_channels: the number of channels in the output of the first conv layer. defaults to 64\n        device: the device to perform the calculations on. defaults to 'cuda'\n        \"\"\" \n        super(Discriminator, self).__init__()\n\n        self.input_size = input_size\n        self.conv1_channels = conv1_channels\n        self.device = device\n\n        self.D = nn.Sequential(\n            nn.Linear(input_size, 128, device=self.device),\n            nn.LeakyReLU(0.1),\n            \n            nn.Linear(128, 100, device=self.device),\n            nn.LeakyReLU(0.1),\n            \n            nn.Linear(100, 64, device=self.device),\n            nn.LeakyReLU(0.1),\n            \n            nn.Linear(64, 1, device=self.device),\n            nn.Sigmoid()\n        )\n\n    \n    def forward(self, x):\n        return self.D(x)","metadata":{"_uuid":"950d4252-d818-479c-bfb4-6905e355ac13","_cell_guid":"620d28e3-a673-4f5d-8b2b-db0213941a9f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-10T20:12:38.019228Z","iopub.execute_input":"2024-01-10T20:12:38.019525Z","iopub.status.idle":"2024-01-10T20:12:38.027460Z","shell.execute_reply.started":"2024-01-10T20:12:38.019498Z","shell.execute_reply":"2024-01-10T20:12:38.026484Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"noise_dim = 784\n\n#generating a random noise matrix (a vector simply doesn't work)\nz = torch.randn(noise_dim, noise_dim, device='cuda')\nprint(z.shape)","metadata":{"_uuid":"7562ef2b-6679-4b27-bfcd-80b4d75a3be1","_cell_guid":"63245739-465c-4b9a-8244-1597f9f4f7b1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-10T20:12:38.465469Z","iopub.execute_input":"2024-01-10T20:12:38.465746Z","iopub.status.idle":"2024-01-10T20:12:38.471057Z","shell.execute_reply.started":"2024-01-10T20:12:38.465721Z","shell.execute_reply":"2024-01-10T20:12:38.470128Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"torch.Size([784, 784])\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"_uuid":"1a7986d9-143b-4579-a88d-0539e55fa967","_cell_guid":"b5f1902d-41cb-4dd7-a5fe-bf6d6889249f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-10T20:12:38.781968Z","iopub.execute_input":"2024-01-10T20:12:38.782755Z","iopub.status.idle":"2024-01-10T20:12:38.787115Z","shell.execute_reply.started":"2024-01-10T20:12:38.782723Z","shell.execute_reply":"2024-01-10T20:12:38.786188Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"from torch.optim.adam import Adam\n\noptimizerG = Adam(G.parameters(), lr=1e-5, betas=[0.9, 0.9])\noptimizerD = Adam(D.parameters(), lr=1e-5, betas=[0.9, 0.9])\n\nloss = nn.BCELoss()","metadata":{"_uuid":"4e14fdc8-5604-46a0-b500-ed85d3c8f9cf","_cell_guid":"d0981e77-b350-42bc-95da-032ec5955110","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-10T20:12:39.071495Z","iopub.execute_input":"2024-01-10T20:12:39.071782Z","iopub.status.idle":"2024-01-10T20:12:39.077500Z","shell.execute_reply.started":"2024-01-10T20:12:39.071756Z","shell.execute_reply":"2024-01-10T20:12:39.076450Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"train_iters = 1000\n\ntorch.autograd.set_detect_anomaly(True)\n\nfor epoch in range(train_iters):\n    ### TRAINING D ON A REAL IMAGE ###\n    \n    z = torch.randn(noise_dim, noise_dim, device='cuda')\n    \n    idx = np.random.randint(len(X_train))\n    \n    D.zero_grad()\n    \n    real_image = X_train[idx].to('cuda')\n        \n    output_real = D.forward(real_image)\n    print(output_real.size())\n    \n    label_real = torch.ones_like(output_real)\n    \n    loss_D_real = loss(output_real, label_real)\n    \n    loss_D_real.backward()\n    \n    \n    ### TRAINING D ON A FAKE IMAGE ###\n    fake = G.forward(z)\n    \n    output_fake = D.forward(fake)\n    \n    D_inp_fake = torch.zeros_like(output_fake)\n\n    loss_D_fake = loss(output_fake, D_inp_fake)\n\n    loss_D = loss_D_real + loss_D_fake\n\n    optimizerD.step()\n    \n    \n    ### TRAINING G ###\n    \n    G.zero_grad()\n    \n    output_D_for_G = D.forward(fake)\n\n    label_fake = torch.zeros_like(output)\n    \n    loss_G = loss(output_D_for_G, label_fake)\n    \n    loss_G.backward()\n    \n    optimizerG.step()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:53:44.684268Z","iopub.execute_input":"2024-01-10T20:53:44.685193Z","iopub.status.idle":"2024-01-10T20:53:44.794197Z","shell.execute_reply.started":"2024-01-10T20:53:44.685155Z","shell.execute_reply":"2024-01-10T20:53:44.792987Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"torch.Size([1, 784, 1])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[101], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m label_fake \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(output)\n\u001b[1;32m     48\u001b[0m loss_G \u001b[38;5;241m=\u001b[39m loss(output_D_for_G, label_fake)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mloss_G\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m optimizerG\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [784, 528]], which is output 0 of LeakyReluBackward1, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"],"ename":"RuntimeError","evalue":"one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [784, 528]], which is output 0 of LeakyReluBackward1, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}